import json
import datetime
from fastapi import FastAPI, HTTPException , Request
from pydantic import BaseModel
from typing import List, Dict
import re
from core.inferencer import Inferencer
import llm_tools.math
import llm_tools.eth

# è®¾ç½® API å¯†é’¥
API_KEY = "123"

# åˆå§‹åŒ– Inferencer
print("Initializing Inferencer...")
tools = []
tools.extend(llm_tools.math.math_tools)
tools.extend(llm_tools.eth.eth_tools)

inferencer = Inferencer(
    model_config={"model_name": "Qwen/Qwen3-1.7B"},
    tools=tools,
    thinking=True
)
print("Inferencer initialized.")

# FastAPI åº”ç”¨
app = FastAPI(
    title="Custom Inferencer API",
    description="Qwen3 + Tools for Open WebUI",
    version="1.0.0"
)

# è¾“å…¥æ•°æ®ç»“æ„
class ChatInput(BaseModel):
    messages: List[Dict[str, str]]
    api_key: str

# æ¨ç†æ¥å£ï¼šåŒæ—¶å…¼å®¹ 2 ä¸ªè·¯å¾„
@app.post("/infer")
@app.post("/infer/chat/completions")
async def infer_openai_compatible(request: Request):
    body = await request.json()
    messages = body.get("messages", [])
    print("ğŸ”¥ OpenAI-style messages:", messages)

    if not messages:
        raise HTTPException(status_code=400, detail="Missing messages")

    if not any(msg["role"] == "system" for msg in messages):
        messages.insert(0, {
            "role": "system",
            "content": f"You are a helpful assistant with tools. {datetime.datetime.now()}"
        })

    response = inferencer.infer(messages)
    last_msg = response[-1]

    # å›æº¯æœ¬è½®æ¶ˆæ¯ï¼Œæ‰¾åˆ°æœ€è¿‘ä¸€æ¬¡ function_call
    function_call_info = ""
    for msg in reversed(response):
        if "function_call" in msg and msg["function_call"]:
            func_name = msg["function_call"].get("name", "")
            args = msg["function_call"].get("arguments", {})
            function_call_info = f"[This answer used Function Call: {func_name} Parameter: {json.dumps(args, ensure_ascii=False)}]"
            break

    # æå–æ€è€ƒå’Œæ­£å¼å›ç­”
    content = last_msg.get("content") or ""
    think_match = re.search(r"<think>(.*?)</think>", content, flags=re.DOTALL)
    thinking = think_match.group(1).strip() if think_match else ""
    # å»é™¤ <think>...</think> å’Œ <|im_end|>
    answer = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL)
    answer = answer.replace("<|im_end|>", "").strip()

    content = ""
    if function_call_info:
        content += function_call_info + "\n\n"
    if thinking:
        content += "ğŸ’¡Model thinkingï¼š\n" + thinking.strip() + "\n\n"
    if answer:
        content += "ğŸ¤–Model answer:\n" + answer.strip()

    last_msg = {
        "role": "assistant",
        "content": content.strip()
    }
    return {"choices": [{"message": last_msg}]}

# æ¨¡å‹åˆ—è¡¨æ¥å£ï¼ˆæ— éœ€å¯†é’¥ï¼‰
@app.get("/infer/models")
async def list_models():
    return {"models": ["Qwen/Qwen3-1.7B"]}
